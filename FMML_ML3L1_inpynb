FOUNDATIONS OF MODERN MACHINE LEARNING, IIIT Hyderabad
MODULE 3: CLASSIFICATION-1
LAB-1 : Understanding Distance metrics and Introduction to KNN
Module Coordinator: Jashn Arora
SECTION - 1 : Distance metrics

[ ]
# Importing Required Libraries
import numpy as np
import pandas as pd
import math
from scipy.spatial import distance
import matplotlib.pyplot as plt
Euclidean Distance
There is more than one way to define distance. The most familiar distance metric is probably Euclidean distance, which is the straight-line distance between the two points. The formula for calculating this distance is a generalization of the Pythagorean theorem:

d(x,x′)=∑j=1D(xj−x′j)2−−−−−−−−−−−⎷

euclidean.png

Image Source


[ ]
# Simple 2D example
x_1 = np.array((1, 2))
x_2 = np.array((4, 6))

[ ]
# Naive approach to calculating Euclidean distance (not-vectorized)
sumv = 0
for i in range(len(x_1)):
  sumv += ((x_1[i]-x_2[i])**2)
dist = math.sqrt(sumv)
print(dist)
5.0

[ ]
## TASK-1
## Write a vectorized version of computing Euclidean distance (using numpy) in the space given below
## (Expected number of lines ~ 1). Your answer should be stored in the variable 'euclidean_dist'
## Verify this result with what you saw above

# Write your code below and uncomment the lines below in order to run the cell.

# euclidean_dist =
# print(euclidean_dist)
Answer to Task-1

[ ]
5.0

[ ]
# Libraries such as SciPy provide functions to compute different kinds of distance metrics between points
euclidean_dist = distance.euclidean(x_1, x_2)
print(euclidean_dist)
5.0

[ ]
# 3-D Visualization
plotx = np.linspace(-10,10,50)
meshx, meshy = np.meshgrid(plotx,plotx)

[ ]
tempeuclid = np.sqrt((meshx**2 + meshy**2))

[ ]
import plotly.graph_objects as go

fig = go.Figure(data=[go.Surface(z=tempeuclid, x=plotx, y=plotx)])
fig.update_layout(title='Euclidean Distance from origin', autosize=False,
                  width=1000, height=1000,
                  margin=dict(l=65, r=50, b=65, t=90))
fig.show()

Manhattan Distance
Euclidean distance is not the only way to measure how far apart two points are. Manhattan distance (also called taxicab distance) measures the distance a taxicab in would have to drive to travel from A to B. Taxicabs cannot travel in a straight line because they have to follow the street grid. But there are multiple paths along the street grid that all have exactly the same length; the Manhattan distance is the length of any one of these shortest paths. The formula for Manhattan distance is as follows:
d(x,x′)=∑j=1D|xj−x′j|

manhattan.png

Image Source


[ ]
# Libraries such as SciPy provide functions to compute different kinds of distance metrics between points
# x_1 = np.array((1, 2))
# x_2 = np.array((4, 6))
manhattan_dist = distance.cityblock(x_1, x_2)
print(manhattan_dist)
7

[ ]
## TASK-2
## Write a vectorized version of computing Manhattan distance (using numpy) in the space given below
## (Expected number of lines ~ 1). Your answer should be stored in the variable 'manhattan_dist'
## Verify your result with the output of the scipy function in the previous cell.

# Write your code below and uncomment the lines below in order to run the cell.

# manhattan_dist =
# print(manhattan_dist)
Answer to Task-2

[ ]
7

[ ]
# 3-D visualization
plotx = np.linspace(-10,10,50)
meshx, meshy = np.meshgrid(plotx,plotx)

[ ]
tempmanhattan = np.abs(meshx) + np.abs(meshy)

[ ]
import plotly.graph_objects as go

fig = go.Figure(data=[go.Surface(z=tempmanhattan, x=plotx, y=plotx)])
fig.update_layout(title='Manhattan Distance from origin', autosize=False,
                  width=1000, height=1000,
                  margin=dict(l=65, r=50, b=65, t=90))
fig.show()

Minkowski Distance

[ ]
↳ 3 cells hidden
Hamming Distance

[ ]
↳ 2 cells hidden
Cosine Similarity

[ ]
↳ 2 cells hidden
Chebyshev Distance

[ ]
↳ 2 cells hidden
Jaccard Distance
The Jaccard distance is a metric that measures dissimilarity between sample sets.
Jaccard Index ie, J(A,B)=|A∩B||A∪B|
Jaccard Distance ie, dJ(A,B)=1−J(A,B)=1−|A∩B||A∪B|


[ ]
↳ 2 cells hidden
Haversine distance
The Haversine formula calculates the shortest distance between two points on a sphere using their latitudes and longitudes measured along the surface. It is important for use in navigation.


[ ]
def haversine(coord1, coord2):

    # Coordinates in decimal degrees (e.g. 2.89078, 12.79797)
    lon1, lat1 = coord1
    lon2, lat2 = coord2

    R = 6371000  # radius of Earth in meters
    phi_1 = math.radians(lat1)
    phi_2 = math.radians(lat2)

    delta_phi = math.radians(lat2 - lat1)
    delta_lambda = math.radians(lon2 - lon1)

    a = math.sin(delta_phi / 2.0) ** 2 + math.cos(phi_1) * math.cos(phi_2) * math.sin(delta_lambda / 2.0) ** 2

    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))

    meters = R * c  # output distance in meters
    km = meters / 1000.0  # output distance in kilometers

    meters = round(meters, 3)
    km = round(km, 3)


    print(f"Distance: " + str(km) + " km")

[ ]
haversine([-0.116773, 51.510357], [-77.009003, 38.889931])
Distance: 5897.658 km
How to decide the appropriate distance metric ?
Euclidean distance: The most widely used metric in KNN classification problems, since it calculates the straight line distance between two points.

Manhattan distance: In some cases, Manhattan distance maybe preferable over Euclidean distances. For eg: For data with high dimensionality, Manhattan distance maybe more useful. When Euclidean distance is defined using many coordinates, there is not a lot of difference in the distances between different pairs of points. This is due to the Curse of Dimensionality, ie, "when the dimensionality increases, the volume of the space increases so fast that the available data become sparse".
Manhattan distance is a more appropriate metric to use if the data is, for example, in the form of a grid. For instance, if the data consists of several houses that are arranged in the form of a grid and connected by roads, Euclidean distance is not an appropriate indicator of the distance between the houses, as in real-life, one would take the roads to travel between two points, thus making Manhattan distance a more suitable metric.

Cosine Similarity: Cosine similarity is commonly used in text-analytics and document comparison problems. It is also used in collaborative filtering-based recommendation systems. It is used when the magnitude of the vectors is not of importance.

Hamming Distance: It is commonly used to measure the distance between categorical variables.

Jaccard Index: The Jaccard index is often used in applications where binary data is used. It can also be used in text similarity analysis to measure how much word choice overlap there is between documents

SECTION - 2 : KNN
What is KNN ?
K-NN (K- Nearest Neighbours) is a classification technique where the output is a class membership.
An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.

What is so unique about KNN ?
First off, KNN is a supervised learning algorithm, ie, the idea is to learn a function that can map an input to an output given some example pairs.

It is a non-parametric algorithm, since it doesn't assume anything about the form of the mapping function, which makes it very flexible to use. Certain algorithms are parametric (such as Naive Bayes), which are constrained as they require a specified form. KNN can, thus, be a good algorithm to try if the form is unknown.

Moreover, KNN is an instance-based algorithm, since it compares new problems/inputs with those which were seen during model training and that were stored in memory.

SECTION - 2.1: KNN on a Synthetic Dataset
In the previous section, you saw different kinds of distance metrics and their values on two sample 2-dimensional points.
This section covers use of distance metrics in a KNN classifier for synthetic two-dimensional data.


[ ]
import random
import scipy.stats as ss
from sklearn.neighbors import KNeighborsClassifier

[ ]
def generate_synth_data(n = 50):
    points = np.concatenate((ss.norm(0, 1).rvs((n, 2)), ss.norm(1, 1).rvs((n, 2))), axis = 0)
    outcomes = np.concatenate((np.repeat(0, n), np.repeat(1, n)))
    return (points, outcomes)

n = 50
pts,tgts = generate_synth_data(n) #generates 100 points
# print(tgts)
plt.figure()
plt.plot(pts[:n, 0], pts[:n, 1], "ro")
plt.plot(pts[n:, 0], pts[n:, 1], "bo")
plt.show()


[ ]
n = 15
test_pts,test_tgts = generate_synth_data(n)
# print(test_tgts)
plt.figure()
plt.plot(test_pts[:n, 0], test_pts[:n, 1], "ro")
plt.plot(test_pts[n:, 0], test_pts[n:, 1], "bo")
plt.show()


[ ]
## TASK - 3
## The code given below is that of a KNN classifier, provided by Scikit-learn.
## The parameter 'metric' includes various distance metric options, including those
## we learnt above- manhattan, euclidean, minkowski,chebyshev, hamming, etc.

## Try out different metrics and observe changes in the accuracy, if any. Don't change value of 'k'
## Also, experiment with different metrics and observe if they are applicable for this kind of data.
## If there are any additional parameters needed, for instance, 'p' in case of Minkowski distance, include them.

knn = KNeighborsClassifier(n_neighbors = 3)
knn.fit(pts, tgts)
our_predictions = knn.predict(test_pts)
print("Prediction Accuracy: ")
print(100 * np.mean(our_predictions == test_tgts))
Prediction Accuracy: 
66.66666666666666
SECTION - 2.2: KNN on a Real World dataset
The Iris flower data set or Fisher's Iris data set is a multivariate data set that consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.


[ ]
from sklearn.datasets import load_iris
iris = load_iris()
data = pd.DataFrame(iris.data, columns=iris.feature_names)
# data['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)
data['target'] = pd.Series(iris.target)
data.head(10)


[ ]
np.random.seed(123)
indices = np.random.permutation(data.shape[0])
div = int(0.65 * len(indices))
train_idx, test_idx = indices[:div], indices[div:]

train_set, test_set = data.loc[train_idx,:], data.loc[test_idx,:]
test_class = list(test_set.iloc[:,-1])
train_class = list(train_set.iloc[:,-1])

[ ]
def dist_euclidean(X1,X2):
    return distance.euclidean(X1,X2)

[ ]
def dist_cosine(X1,X2):
    return distance.cosine(X1,X2)

[ ]
def dist_manhattan(X1,X2):
    return distance.cityblock(X1,X2)

[ ]
def dist_chebyshev(X1,X2):
    return distance.chebyshev(X1,X2)

[ ]
metrics_list = [dist_euclidean,dist_cosine, dist_manhattan, dist_chebyshev]
for dist_metric in metrics_list:
  knn = KNeighborsClassifier(n_neighbors = 3, metric=dist_metric)
  knn.fit(train_set, train_class)
  our_predictions = knn.predict(test_set)
  print(dist_metric.__name__)
  print("Prediction Accuracy: ")
  print(100 * np.mean(our_predictions == test_class))
  print()
dist_euclidean
Prediction Accuracy: 
100.0

dist_cosine
Prediction Accuracy: 
100.0

dist_manhattan
Prediction Accuracy: 
100.0

dist_chebyshev
Prediction Accuracy: 
100.0

Questions to Think About and Answer:
How are similarity and distance different from each other?
**Similarity** and **distance** are both measures used to quantify how related or how far apart two data points are, but they approach the problem in opposite directions and have different mathematical properties.

### 1. **Distance**:
Distance measures the **degree of separation** between two points. It quantifies how far apart the points are in a given space. The larger the distance, the further apart the points are.

- **Mathematical Definition**: Distance is typically a **non-negative** value, and in most cases, it satisfies the properties of a metric:
  1. **Non-negativity**: The distance between two points is always zero or positive.
  2. **Symmetry**: The distance between point A and point B is the same as the distance from point B to point A.
  3. **Triangle Inequality**: The distance between two points A and C is less than or equal to the sum of the distance from A to B and from B to C.
  4. **Identity of indiscernibles**: The distance between two points is zero if and only if the points are identical.

- **Common Distance Metrics**:
  - **Euclidean Distance**: Measures the "straight-line" distance between two points in Euclidean space (e.g., 2D or 3D space).
    \[
    \text{Euclidean Distance} = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}
    \]
  - **Manhattan Distance**: Also called "taxicab" distance, it is the sum of the absolute differences of the coordinates.
    \[
    \text{Manhattan Distance} = |x_1 - x_2| + |y_1 - y_2|
    \]
  - **Cosine Distance**: Measures the distance based on the angle between two vectors (this is related to cosine similarity, described below).

- **Example**: If two points are far apart in space (say in terms of Euclidean distance), they are considered **distant** from each other.

### 2. **Similarity**:
Similarity measures how **alike** two points are, based on some shared characteristics or patterns. A higher similarity means the points are more alike, while a lower similarity means they are less alike.

- **Mathematical Definition**: Similarity is generally **normalized** to be between 0 and 1 (although in some cases, it can be between -1 and 1). In other words, **high similarity** means that two points are **closer to each other** in terms of features, while **low similarity** indicates that the points are different.

- **Common Similarity Measures**:
  - **Cosine Similarity**: Measures the cosine of the angle between two vectors. It is often used in text mining and machine learning.
    \[
    \text{Cosine Similarity} = \frac{\text{A} \cdot \text{B}}{\|A\| \|B\|}
    \]
    where \(A\) and \(B\) are vectors, and \(\|A\|\) and \(\|B\|\) are their magnitudes.
  - **Pearson Correlation**: Measures the linear correlation between two variables. It ranges from -1 (perfect negative correlation) to 1 (perfect positive correlation).
    \[
    \text{Pearson Correlation} = \frac{\text{Cov}(A, B)}{\sigma_A \sigma_B}
    \]
  - **Jaccard Similarity**: Measures similarity for binary or categorical data, defined as the size of the intersection divided by the size of the union of the sets.
    \[
    \text{Jaccard Similarity} = \frac{|A \cap B|}{|A \cup B|}
    \]

- **Example**: In a dataset of customer preferences, if two customers have very similar preferences (e.g., both buy the same items), they will have a **high similarity**.

### Key Differences:

| Feature               | **Distance**                                      | **Similarity**                                      |
|-----------------------|--------------------------------------------------|----------------------------------------------------|
| **Concept**           | Measures how far apart two points are            | Measures how similar or alike two points are       |
| **Value Range**       | Typically non-negative values, often \( \geq 0 \) | Ranges from 0 to 1 (or -1 to 1 for some measures) |
| **Direction**         | Larger distance = further apart                   | Larger similarity = more similar                  |
| **Zero**              | Distance of 0 means the points are identical      | Similarity of 1 means the points are identical     |
| **Example Metric**    | Euclidean Distance, Manhattan Distance           | Cosine Similarity, Jaccard Similarity, Pearson Correlation |

### Relationship Between Similarity and Distance:
- In many cases, **distance** and **similarity** are inversely related. As the distance between two points increases, their similarity tends to decrease, and vice versa.
- For example, in **cosine similarity**, when the cosine value is close to 1 (indicating a small angle between the vectors), the **distance** between the vectors will be small (and the points are similar). When the cosine similarity approaches 0, the vectors are orthogonal and far apart.

### Conclusion:
- **Distance** is a measure of how **far apart** two points are in space, whereas **similarity** quantifies how **alike** or **related** two points are.
- **Distance** tends to focus on global relationships (absolute positions in space), while **similarity** tends to focus on the relationship between points based on patterns or structure, often used in clustering and classification tasks.

Are there any conditions for a particular distance to be considered a "distance metric"?
Yes, for a particular measure of distance to be considered a **valid distance metric** in mathematics, it must satisfy the following four conditions, which collectively define a **metric space**:

### 1. **Non-negativity (Positivity):**
   \[
   d(x, y) \geq 0 \quad \text{for all } x, y \in X
   \]
   The distance between any two points \( x \) and \( y \) must always be a non-negative value.

### 2. **Identity of Indiscernibles:**
   \[
   d(x, y) = 0 \iff x = y
   \]
   The distance between two points is zero if and only if the two points are the same.

### 3. **Symmetry:**
   \[
   d(x, y) = d(y, x) \quad \text{for all } x, y \in X
   \]
   The distance from \( x \) to \( y \) must be the same as the distance from \( y \) to \( x \).

### 4. **Triangle Inequality:**
   \[
   d(x, z) \leq d(x, y) + d(y, z) \quad \text{for all } x, y, z \in X
   \]
   The direct distance between two points \( x \) and \( z \) must be less than or equal to the sum of the distances from \( x \) to \( y \) and \( y \) to \( z \). This ensures that the shortest path between two points is a straight line.

---

### Common Examples of Distance Metrics
- **Euclidean Distance**: \(\sqrt{\sum_{i=1}^n (x_i - y_i)^2}\)
- **Manhattan Distance**: \(\sum_{i=1}^n |x_i - y_i|\)
- **Chebyshev Distance**: \(\max_i |x_i - y_i|\)
- **Hamming Distance**: The number of positions at which two strings of equal length differ.

If a measure satisfies all four conditions, it is considered a distance metric. If it violates any condition, it may still be a useful measure but is not a metric.
Useful Resources for further reading
Analytics Vidhya: Distance metrics
Scikit learn distance metric documentation
